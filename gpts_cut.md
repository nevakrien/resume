# Neva Krien  
**Email**: [nevo.krien@gmail.com](mailto:nevo.krien@gmail.com)  
**GitHub**: [Neva Krien's GitHub](https://github.com/nevakrien)  

---

## **Summary**  
AI researcher with 2+ years of experience in developing cutting-edge LLM technologies, including optimizing code for Intel GPUs and creating tools for efficient AI development. Published author with hands-on expertise in Python, TensorFlow, and Huggingface, contributing to high-impact projects and tutorials.

---

## **Technical Skills**  
- **Programming Languages**: Python, Rust, C, C++, Go, Elixir, Java, NASM.  
- **AI Frameworks/Tools**: Huggingface, TensorFlow, PyTorch, OpenVINO, LangChain, Faiss.  
- **Backend Development**: Flask, SQL, PostgreSQL.  
- **Frontend Development**: HTML, CSS, JavaScript, Raylib.  
- **Others**: CUDA, LLVM IR, x86-64 Assembly.  

---

## **Professional Experience**  

## **AI Developer**  
**Dates**: 2022 – Present  

- **Educational Content Creation**: *Intel*  
  Since 2022, I have developed educational content explaining AI development concepts and technologies.  
  My work includes writing code featured in Intel’s official [YouTube tutorials](https://www.youtube.com/watch?v=6-ToSvHidy0&list=PLXB3P9W0qDDhtDAjs1U2arVWiyq9z3nDS&index=5) and internal workshops.  
  I specialize in breaking down complex AI topics and new research into digestible formats for both technical and non-technical audiences. Over the course of my work, I have explained over **100 research papers, topics, and technologies**, tailoring content for various audiences.  
  Additionally, I wrote inference code optimized for consumer laptops, achieving up to **10x reliability improvements** through OpenVINO optimizations.

- **Research Contributions**: *Intel and Technion Collaboration*  
  As part of a collaborative team between Intel and Technion, I worked on developing code generation LLMs for C, C++, and Fortran.  
  Initially recruited for my mathematical expertise, I expanded my responsibilities to include implementing and debugging robust LLM evaluation pipelines and optimizing model performance for Intel GPUs.  
  I conducted a thorough literature review, analyzing over **10 related research papers within a week** to support the development of our LLM framework and ensure alignment with state-of-the-art methodologies.  
  **Publication**: [Code Generation with LLMs](https://arxiv.org/abs/2308.09440)  


---

## **Projects**  
- **Compiler Development**:  
  Developed two compilers using C99, C++-17, and unsafe Rust ([Faeyne_lang](https://github.com/nevakrien/Faeyne_lang), [Turing Compiler](https://github.com/nevakrien/Turing-compiler)).   
  Constant folding and memory alignment achieved over 2x performance improvement from the baseline.  
  Tail call optimization moved from O(n) to O(1) memory usage achiving over 100x   improvment on common test cases.

- **AI Assistant Website**:  
  Built an AI assistant using OpenAI API with custom prompt engineering logic ([GitHub](https://github.com/nevakrien/ai_secretary2)).  
  Designed the architecture for efficient API interactions and custom workflows.

- **Movie Subtitle Translation Tool**:  
  Created a tool that automates subtitle translations ([GitHub](https://github.com/nevakrien/srt_translate)).  
  Saved an estimated 20 hours of manual translation work for 1 hour of development time.

- **VPN Proxy**:  
  Created a tool to manage VPN proxying for web scraping tasks ([GitHub](https://github.com/nevakrien/vpn_proxy)), allowing the use of six rotating addresses instead of one.

---

## **Education**  
**Bachelor’s Degree**  
*The Open University of Israel*  
**Status**: Second Year  

---

## **References**  
**Guy Tamir**  
Intel Researcher  
Email: [guy.tamir@intel.com](mailto:guy.tamir@intel.com)  
